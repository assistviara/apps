# app.py â€” PCAå¯¾å¿œç‰ˆï¼šExcel/Sheetså…¥åŠ› â†’ å‰å‡¦ç† â†’ ä¸»æˆåˆ†åˆ†æï¼ˆSVDï¼‰ â†’ å¯è¦–åŒ–
# Author: ãŸã‘ã—ã‚ƒã‚“ç”¨ï¼ˆ2025-08ï¼‰
import os, re, json, unicodedata
from pathlib import Path

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from matplotlib import font_manager, rcParams
from matplotlib.patches import Rectangle
from dotenv import load_dotenv

# --------- 1st Streamlit call must be set_page_config ----------
st.set_page_config(
    page_title="é£²é£Ÿåº—è©•ä¾¡ï¼šPCA & ãƒãƒˆãƒªã‚¯ã‚¹",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ===== .env =====
load_dotenv()
DEFAULT_SHEET_ID = os.getenv("GOOGLE_SHEET_ID", "")
DEFAULT_WS_NAME  = os.getenv("GSHEET_WORKSHEET", "Form Responses")
DEFAULT_SVC_JSON = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON", "")
DEFAULT_SVC_JSON_PATH = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON_PATH", "")

# ===== JP font (optional) =====
FONT_DIR = Path(__file__).parent / "fonts"
JP_FONT = FONT_DIR / "NotoSansJP-Regular.ttf"
try:
    if JP_FONT.exists():
        font_manager.fontManager.addfont(str(JP_FONT))
        try: font_manager._rebuild()
        except Exception: pass
        jp_name = font_manager.FontProperties(fname=str(JP_FONT)).get_name()
        rcParams["font.family"] = "sans-serif"
        rcParams["font.sans-serif"] = [jp_name, "DejaVu Sans", "Arial", "Liberation Sans"]
    else:
        rcParams["font.family"] = "DejaVu Sans"
    rcParams["axes.unicode_minus"] = False
except Exception:
    rcParams["font.family"] = "DejaVu Sans"
    rcParams["axes.unicode_minus"] = False

# ===== Old-matrix columns (optional) =====
DIVERSITY_COLS = [
    "å¤šæ§˜æ€§1_ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§","å¤šæ§˜æ€§2_å†…è£…ã®å€‹æ€§","å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©","å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "å¤šæ§˜æ€§5_åœ°åŸŸæ€§ã®åæ˜ ","å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€","å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•","å¤šæ§˜æ€§8_å®¢å±¤ã®å¤šæ§˜æ€§",
    "å¤šæ§˜æ€§9_æä¾›æ–¹æ³•ã®ç‰¹ç•°æ€§","å¤šæ§˜æ€§10_åº—ã®ç‰©èªæ€§"
]
BRAND_COLS = [
    "é˜²è¡›1_å‘³ã®ä¿¡é ¼æ„Ÿï¼ˆåˆè¨ªï¼‰","é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ","é˜²è¡›3_æ¥å®¢æ…‹åº¦","é˜²è¡›4_ä¾¡æ ¼ã®æ˜ç¢ºã•",
    "é˜²è¡›5_æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰","é˜²è¡›6_æ”¯æ‰•ã„ã®å®‰å…¨æ€§","é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•","é˜²è¡›8_åˆè¦‹å®¢ã¸ã®å¯¾å¿œ",
    "é˜²è¡›9_å¸¸é€£/å£ã‚³ãƒŸ","é˜²è¡›10_ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›"
]
MIDLINE = 30
MIN_SCORE, MAX_SCORE = 1, 5

# ===== Normalization / Aliases =====
def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", str(s))
    return s.replace(" ", "").replace("ã€€", "").lower()

ALIAS_COLS = {
    "åº—å": ["åº—å","ãŠåº—å","åº—èˆ—å","ã‚·ãƒ§ãƒƒãƒ—å","åº—èˆ—"],
    "æ—¥ä»˜": ["æ—¥ä»˜","è¨ªå•æ—¥","æ¥åº—æ—¥","æ—¥æ™‚"],
    "è©•ä¾¡é …ç›®": ["è©•ä¾¡é …ç›®","é …ç›®","è³ªå•","è³ªå•æ–‡"],
    "ã‚¹ã‚³ã‚¢": ["ã‚¹ã‚³ã‚¢","ç‚¹æ•°","è©•ä¾¡","score","å¾—ç‚¹"],
    "ã‚³ãƒ¡ãƒ³ãƒˆ": ["ã‚³ãƒ¡ãƒ³ãƒˆ","è‡ªç”±è¨˜è¿°","ãƒ¡ãƒ¢","å‚™è€ƒ","è‡ªç”±å›ç­”"],
    "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": ["section","ã‚»ã‚¯ã‚·ãƒ§ãƒ³","åŒºåˆ†","ã‚«ãƒ†ã‚´ãƒª","ã‚«ãƒ†ã‚´ãƒªãƒ¼"]
}

# â˜…é‡è¦ï¼šå®Œå…¨ä¸€è‡´ã®ã¿ï¼æ—§ãƒãƒˆãƒªã‚¯ã‚¹ã®â€œæ­£ç¢ºãªè¦‹å‡ºã—ã ã‘â€ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
NORMALIZE_RULES = {
    # meta
    "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—",
    "è¨ªå•æ—¥": "æ—¥ä»˜",
    "ãŠåº—å": "åº—å",
    "åº—å": "åº—å",
    "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰": "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰",

    # å¤šæ§˜æ€§ï¼ˆæ—§ãƒãƒˆãƒªã‚¯ã‚¹ï¼‰
    "å¤šæ§˜æ€§1_ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§": "å¤šæ§˜æ€§1_ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§",
    "å¤šæ§˜æ€§2_å†…è£…ã®å€‹æ€§": "å¤šæ§˜æ€§2_å†…è£…ã®å€‹æ€§",
    "å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©": "å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©",
    "å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§": "å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "å¤šæ§˜æ€§5_åœ°åŸŸæ€§ã®åæ˜ ": "å¤šæ§˜æ€§5_åœ°åŸŸæ€§ã®åæ˜ ",
    "å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€": "å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€",
    "å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•": "å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•",
    "å¤šæ§˜æ€§8_å®¢å±¤ã®å¤šæ§˜æ€§": "å¤šæ§˜æ€§8_å®¢å±¤ã®å¤šæ§˜æ€§",
    "å¤šæ§˜æ€§9_æä¾›æ–¹æ³•ã®ç‰¹ç•°æ€§": "å¤šæ§˜æ€§9_æä¾›æ–¹æ³•ã®ç‰¹ç•°æ€§",
    "å¤šæ§˜æ€§10_åº—ã®ç‰©èªæ€§": "å¤šæ§˜æ€§10_åº—ã®ç‰©èªæ€§",

    # é˜²è¡›ï¼ˆæ—§ãƒãƒˆãƒªã‚¯ã‚¹ï¼‰
    "é˜²è¡›1_å‘³ã®ä¿¡é ¼æ„Ÿï¼ˆåˆè¨ªï¼‰": "é˜²è¡›1_å‘³ã®ä¿¡é ¼æ„Ÿï¼ˆåˆè¨ªï¼‰",
    "é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ": "é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ",
    "é˜²è¡›3_æ¥å®¢æ…‹åº¦": "é˜²è¡›3_æ¥å®¢æ…‹åº¦",
    "é˜²è¡›4_ä¾¡æ ¼ã®æ˜ç¢ºã•": "é˜²è¡›4_ä¾¡æ ¼ã®æ˜ç¢ºã•",
    "é˜²è¡›5_æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰": "é˜²è¡›5_æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰",
    "é˜²è¡›6_æ”¯æ‰•ã„ã®å®‰å…¨æ€§": "é˜²è¡›6_æ”¯æ‰•ã„ã®å®‰å…¨æ€§",
    "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "é˜²è¡›8_åˆè¦‹å®¢ã¸ã®å¯¾å¿œ": "é˜²è¡›8_åˆè¦‹å®¢ã¸ã®å¯¾å¿œ",
    "é˜²è¡›9_å¸¸é€£/å£ã‚³ãƒŸ": "é˜²è¡›9_å¸¸é€£/å£ã‚³ãƒŸ",
    "é˜²è¡›10_ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›": "é˜²è¡›10_ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›",
}

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """å®Œå…¨ä¸€è‡´ã®ã¿ã§æ­£è¦åŒ–ï¼ˆæ—§ãƒãƒˆãƒªã‚¯ã‚¹è¦‹å‡ºã—ã®å®‰å…¨ãªåŒå®šï¼‰"""
    new_cols = []
    for c in df.columns:
        cn = _norm(c)
        mapped = None
        for key, dest in NORMALIZE_RULES.items():
            if _norm(key) == cn:
                mapped = dest
                break
        new_cols.append(mapped or c)
    df.columns = new_cols
    return df

def find_col(df: pd.DataFrame, logical_name: str) -> str | None:
    cands = ALIAS_COLS.get(logical_name, [])
    cols_norm = { _norm(c): c for c in df.columns }
    for key in cands:
        k = _norm(key)
        for cn, orig in cols_norm.items():
            if k in cn:
                return orig
    return None

# ===== Robust score conversion =====
def _to_1to5(x):
    if isinstance(x, (pd.Series, pd.DataFrame)):
        return x.applymap(_to_1to5) if isinstance(x, pd.DataFrame) else x.apply(_to_1to5)
    if pd.isna(x): return np.nan
    s = unicodedata.normalize("NFKC", str(x)).strip()
    if s == "": return np.nan
    likert_map = {
        "éå¸¸ã«ä½ã„":1, "ã¨ã¦ã‚‚ä½ã„":1, "ä½ã„":2, "ã‚„ã‚„ä½ã„":2,
        "ãµã¤ã†":3, "æ™®é€š":3, "ã‚„ã‚„é«˜ã„":4, "é«˜ã„":4, "éå¸¸ã«é«˜ã„":5, "ã¨ã¦ã‚‚é«˜ã„":5
    }
    if s in likert_map: return float(likert_map[s])
    m = re.search(r"([0-9]+)", s)
    if m:
        v = int(m.group(1))
        if 5 < v <= 100: v = round(v/20)
        return float(max(1, min(5, v)))
    try:
        v = float(s)
        return float(max(1, min(5, v)))
    except:
        return np.nan

def coerce_1to5(df: pd.DataFrame) -> pd.DataFrame:
    for c in df.columns:
        if any(kw in str(c) for kw in ["ã‚³ãƒ¡ãƒ³ãƒˆ","è‡ªç”±è¨˜è¿°","å‚™è€ƒ","ãƒ¡ãƒ¢"]): continue
        if c in ("åº—å","æ—¥ä»˜","ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—"): continue
        df[c] = df[c].apply(_to_1to5)
    return df

def drop_unnamed_columns(df: pd.DataFrame) -> pd.DataFrame:
    keep = [c for c in df.columns if not str(c).startswith("Unnamed:")]
    return df.loc[:, keep]

def collapse_duplicate_columns(df: pd.DataFrame, agg: str = "mean") -> pd.DataFrame:
    if df.columns.has_duplicates:
        new_data = {}
        for name in df.columns.unique():
            block = df.loc[:, df.columns == name]
            if block.shape[1] == 1:
                new_data[name] = block.iloc[:, 0]
            else:
                block_num = block.apply(pd.to_numeric, errors="coerce")
                if agg == "mean":
                    new_series = block_num.mean(axis=1, skipna=True)
                elif agg == "max":
                    new_series = block_num.max(axis=1, skipna=True)
                elif agg == "min":
                    new_series = block_num.min(axis=1, skipna=True)
                else:
                    new_series = block_num.mean(axis=1, skipna=True)
                new_data[name] = new_series
        df = pd.DataFrame(new_data)
    return df

# â˜…æœ€çµ‚é™¤æŸ“ï¼ˆã“ã“ãŒè‚ï¼‰
def sanitize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = drop_unnamed_columns(df)
    clean = []
    for c in df.columns:
        cc = unicodedata.normalize("NFKC", str(c)).strip().rstrip("ï¼š:").strip()
        clean.append(cc)
    df.columns = clean
    df = normalize_columns(df)                 # å®Œå…¨ä¸€è‡´ã®ã¿
    df = collapse_duplicate_columns(df, "mean")
    if df.columns.duplicated().any():          # ã¾ã é‡è¤‡ãŒæ®‹ã‚‹ãªã‚‰å¼·åˆ¶ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆå®‰å…¨å¼ï¼‰
        cols, seen = [], {}
        for c in df.columns:
            if c not in seen:
                seen[c] = 1; cols.append(c)
            else:
                seen[c] += 1; cols.append(f"{c}__dup{seen[c]}")
        df.columns = cols
    return df

# ===== Longâ†’Wide =====
def wide_from_long(df_long: pd.DataFrame) -> pd.DataFrame:
    col_store = find_col(df_long, "åº—å")
    col_date  = find_col(df_long, "æ—¥ä»˜")
    col_item  = find_col(df_long, "è©•ä¾¡é …ç›®")
    col_score = find_col(df_long, "ã‚¹ã‚³ã‚¢")
    assert all([col_store, col_date, col_item, col_score]), "ç¸¦æŒã¡â†’æ¨ªæŒã¡å¤‰æ›ã«å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    df_use = df_long[[col_store, col_date, col_item, col_score]].copy()
    df_use[col_score] = df_use[col_score].apply(_to_1to5)
    wide = df_use.pivot_table(index=[col_store, col_date], columns=col_item, values=col_score, aggfunc="mean")
    wide = wide.reset_index(); wide.columns.name = None
    wide = wide.rename(columns={col_store:"åº—å", col_date:"æ—¥ä»˜"})
    return coerce_1to5(wide)

# ===== Readers =====
def read_from_excel(file) -> pd.DataFrame:
    df = pd.read_excel(file).dropna(how="all")
    df = drop_unnamed_columns(df)
    if find_col(df, "è©•ä¾¡é …ç›®") and find_col(df, "ã‚¹ã‚³ã‚¢"):
        df = wide_from_long(df)
    else:
        df = normalize_columns(df)
        alt = find_col(df, "åº—å")
        if alt and alt != "åº—å": df = df.rename(columns={alt: "åº—å"})
        alt = find_col(df, "æ—¥ä»˜")
        if alt and alt != "æ—¥ä»˜": df = df.rename(columns={alt: "æ—¥ä»˜"})
        df = coerce_1to5(df)
    df = sanitize_columns(df)   # â˜…æœ€å¾Œã«å¿…ãšæœ€çµ‚é™¤æŸ“
    return df

def extract_sheet_id(text: str) -> str:
    t = (text or "").strip()
    m = re.search(r"/d/([a-zA-Z0-9-_]+)/?", t)
    return m.group(1) if m else t

def read_from_sheets(creds_dict, sheet_id, worksheet) -> pd.DataFrame:
    import gspread
    from google.oauth2.service_account import Credentials
    from gspread_dataframe import get_as_dataframe

    creds = Credentials.from_service_account_info(
        creds_dict, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
    )
    gc = gspread.authorize(creds)
    sid = extract_sheet_id(sheet_id)
    sh = gc.open_by_key(sid)
    try:
        ws = sh.worksheet(worksheet)
    except Exception:
        ws = sh.worksheets()[0]

    df = get_as_dataframe(ws, evaluate_formulas=True, header=0).dropna(how="all")
    df = drop_unnamed_columns(df)
    if find_col(df, "è©•ä¾¡é …ç›®") and find_col(df, "ã‚¹ã‚³ã‚¢"):
        df = wide_from_long(df)
    else:
        df = normalize_columns(df)
        alt = find_col(df, "åº—å")
        if alt and alt != "åº—å": df = df.rename(columns={alt: "åº—å"})
        alt = find_col(df, "æ—¥ä»˜")
        if alt and alt != "æ—¥ä»˜": df = df.rename(columns={alt: "æ—¥ä»˜"})
        df = coerce_1to5(df)
    df = sanitize_columns(df)   # â˜…æœ€å¾Œã«å¿…ãšæœ€çµ‚é™¤æŸ“
    return df

# ===== PCA (SVD) =====
def pca_svd(df_items: pd.DataFrame):
    X = df_items.copy()
    for c in X.columns:
        col = pd.to_numeric(X[c], errors="coerce")
        m = col.mean(skipna=True)
        X[c] = col.fillna(m)
    X = X.loc[:, X.var() > 1e-12]
    X = X.loc[:, ~X.T.duplicated()]
    mu = X.mean(axis=0)
    sd = X.std(axis=0, ddof=1).replace(0, 1.0)
    Z = (X - mu) / sd
    Z = Z.values
    U, S, VT = np.linalg.svd(Z, full_matrices=False)
    n_samples = Z.shape[0]
    eigvals = (S**2) / (n_samples - 1) if n_samples > 1 else (S**2)
    ev_ratio = eigvals / eigvals.sum() if eigvals.sum() > 0 else np.zeros_like(eigvals)
    scores = U * S
    loadings = VT.T
    scores_df = pd.DataFrame(scores, columns=[f"PC{i+1}" for i in range(scores.shape[1])])
    loadings_df = pd.DataFrame(loadings, index=X.columns, columns=[f"PC{i+1}" for i in range(loadings.shape[1])])
    return scores_df, loadings_df, eigvals, ev_ratio

# ===== Old matrix plot (optional) =====
def draw_matrix_plot(df: pd.DataFrame, show_all: bool, show_labels: bool, max_labels: int):
    fig, ax = plt.subplots(figsize=(9, 6), dpi=120)
    ok_vals = {"yes","y","true","1","ok","â—‹","ã¯ã„","å¯"}
    mask = df.get("å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰", pd.Series(["ã¯ã„"]*len(df))).astype(str).str.strip().str.lower().isin(ok_vals)
    plot_df = df.copy() if show_all else df[mask].copy()
    ax.add_patch(Rectangle((0, 0), 50, 50, facecolor=(0,0,0,0.02), edgecolor="none"))
    ax.add_patch(Rectangle((MIDLINE, 0), 50-MIDLINE, 50, facecolor=(0,0,0,0.04), edgecolor="none"))
    ax.add_patch(Rectangle((0, MIDLINE), 50, 50-MIDLINE, facecolor=(0,0,0,0.04), edgecolor="none"))
    ax.scatter(plot_df["å¤šæ§˜æ€§åˆè¨ˆ"], plot_df["é˜²è¡›åˆè¨ˆ"], s=64, alpha=0.9, linewidths=0.6, edgecolors="white")
    ax.axvline(MIDLINE, lw=1); ax.axhline(MIDLINE, lw=1)
    ax.set_xlim(0, 50); ax.set_ylim(0, 50)
    ax.set_xlabel("å¤šæ§˜æ€§åˆè¨ˆï¼ˆ1ã€œ5Ã—10ï¼10ã€œ50ï¼‰")
    ax.set_ylabel("ãƒ–ãƒ©ãƒ³ãƒ‰é˜²è¡›åˆè¨ˆï¼ˆ1ã€œ5Ã—10ï¼10ã€œ50ï¼‰")
    ax.set_title("é£²é£Ÿåº—ã‚¹ã‚³ã‚¢ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆå‚è€ƒï¼‰")
    if show_labels and not plot_df.empty:
        label_df = plot_df.sort_values(["å¤šæ§˜æ€§åˆè¨ˆ","é˜²è¡›åˆè¨ˆ"], ascending=False).head(max_labels)
        for _, r in label_df.iterrows():
            ax.annotate(str(r["åº—å"]), (r["å¤šæ§˜æ€§åˆè¨ˆ"], r["é˜²è¡›åˆè¨ˆ"]),
                        xytext=(4, 4), textcoords="offset points", fontsize=9)
    st.pyplot(fig, clear_figure=True)
    shown = plot_df.loc[:, ["åº—å","å¤šæ§˜æ€§åˆè¨ˆ","é˜²è¡›åˆè¨ˆ"]].sort_values(["é˜²è¡›åˆè¨ˆ","å¤šæ§˜æ€§åˆè¨ˆ"], ascending=False)
    st.dataframe(shown, use_container_width=True)

# ===== UI =====
st.title("é£²é£Ÿåº—è©•ä¾¡ï¼šä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ & ãƒãƒˆãƒªã‚¯ã‚¹")

with st.sidebar:
    st.header("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
    source = st.radio("é¸æŠ", ["Excelã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"], index=0, key="source_kind")
    uploaded = None
    creds_dict = None
    sheet_id_input = ""
    ws_name_input = ""
    if source == "Excelã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        uploaded = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsxï¼‰ã‚’é¸æŠ", type=["xlsx"], key="xlsx_uploader")
        st.caption("ç¸¦æŒã¡ï¼ˆSection/è©•ä¾¡é …ç›®/ã‚¹ã‚³ã‚¢â€¦ï¼‰ã§ã‚‚æ¨ªæŒã¡ï¼ˆå„é …ç›®ãŒåˆ—ï¼‰ã§ã‚‚OKã€‚")
    else:
        st.caption("â€» ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«å¯¾è±¡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–²è¦§å…±æœ‰ã—ã¦ãã ã•ã„ã€‚")
        sheet_id_input = st.text_input("Spreadsheet ID / URL", value=DEFAULT_SHEET_ID, key="sheet_id")
        ws_name_input = st.text_input("Worksheetåï¼ˆã‚¿ãƒ–åï¼‰", value=DEFAULT_WS_NAME, key="worksheet_name")
        svc_default_text = DEFAULT_SVC_JSON or (Path(DEFAULT_SVC_JSON_PATH).read_text(encoding="utf-8") if DEFAULT_SVC_JSON_PATH and Path(DEFAULT_SVC_JSON_PATH).exists() else "")
        svc_text = st.text_area("Service Account JSONï¼ˆè²¼ã‚Šä»˜ã‘ï¼‰", value=svc_default_text, height=160, key="svc_json")
        if svc_text.strip():
            try:
                creds_dict = json.loads(svc_text); st.success("ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
            except Exception as e:
                st.error(f"JSONè§£æã«å¤±æ•—: {e}")

    st.header("PCA è¨­å®š")
    show_vectors = st.checkbox("é …ç›®ãƒ™ã‚¯ãƒˆãƒ«ã‚’é‡ã­æç”»ï¼ˆæœ€å¤§15ï¼‰", value=True, key="show_vectors")
    max_vec = st.slider("ãƒ™ã‚¯ãƒˆãƒ«ã®æœ€å¤§è¡¨ç¤ºæœ¬æ•°", 0, 30, 15, 1, key="max_vec")

    st.header("å‚è€ƒï¼šåˆè¨ˆç‚¹ãƒãƒˆãƒªã‚¯ã‚¹")
    show_matrix = st.checkbox("æ—§ãƒãƒˆãƒªã‚¯ã‚¹ã‚‚æã", value=False, key="show_matrix")
    show_all = st.checkbox("å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç„¡è¦–ï¼ˆå…¨ä»¶ï¼‰", value=False, key="show_all")
    show_labels = st.checkbox("åº—åãƒ©ãƒ™ãƒ«ï¼ˆãƒãƒˆãƒªã‚¯ã‚¹ï¼‰", value=True, key="show_labels")
    max_labels = st.slider("ãƒ©ãƒ™ãƒ«æœ€å¤§ä»¶æ•°ï¼ˆãƒãƒˆãƒªã‚¯ã‚¹ï¼‰", 0, 200, 50, 5, key="max_labels")

go = st.button("PCAã‚’å®Ÿè¡Œ", type="primary", key="run_pca")

def extract_sheet_id(text: str) -> str:
    t = (text or "").strip()
    m = re.search(r"/d/([a-zA-Z0-9-_]+)/?", t)
    return m.group(1) if m else t

# ===== Run =====
if go:
    try:
        if source == "Excelã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
            if not uploaded:
                st.error("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"); st.stop()
            df_raw = read_from_excel(uploaded)
        else:
            if not creds_dict or not sheet_id_input:
                st.error("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®è¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"); st.stop()
            df_raw = read_from_sheets(creds_dict, sheet_id_input, ws_name_input)

        # â˜…èª­ã¿è¾¼ã¿ç›´å¾Œã«æœ€çµ‚é™¤æŸ“ï¼ˆã“ã“ãŒåŠ¹ãï¼ï¼‰
        df_raw = sanitize_columns(df_raw)

        # Debug (ä¸€æ™‚)ï¼šåˆ—åã®è¦‹ãˆã‚‹åŒ–
        st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­10è¡Œï¼‰")
        st.dataframe(df_raw.head(10), use_container_width=True)
        st.caption(f"è¡Œæ•°: {len(df_raw)} / åˆ—æ•°: {len(df_raw.columns)}")

        # å¿…é ˆãƒ¡ã‚¿
        if "åº—å" not in df_raw.columns:
            st.error("åº—å åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ ã« åº—å ã‚’å«ã‚ã¦ãã ã•ã„ã€‚"); st.stop()
        if "æ—¥ä»˜" not in df_raw.columns:
            df_raw["æ—¥ä»˜"] = pd.NaT

        # æ•°å€¤åˆ—ï¼ˆè‡ªç”±è¨˜è¿°ãƒ»ãƒ¡ã‚¿é™¤å¤–ï¼‰
        meta_cols = ["åº—å","æ—¥ä»˜","ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—","å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰"]
        numeric_cols = [c for c in df_raw.columns
                        if c not in meta_cols
                        and not any(kw in str(c) for kw in ["ã‚³ãƒ¡ãƒ³ãƒˆ","è‡ªç”±è¨˜è¿°","å‚™è€ƒ","ãƒ¡ãƒ¢"])
                        and pd.api.types.is_numeric_dtype(df_raw[c])]

        if len(numeric_cols) < 3:
            st.error(f"æ•°å€¤ã®è©•ä¾¡é …ç›®ãŒå°‘ãªã™ãã¾ã™ï¼ˆè¦‹ã¤ã‹ã£ãŸæ•°: {len(numeric_cols)}ã€3åˆ—ä»¥ä¸ŠãŒæœ›ã¾ã—ã„ï¼‰ã€‚"); st.stop()

        df_items = df_raw[numeric_cols].copy()
        scores_df, loadings, ev, ev_ratio = pca_svd(df_items)

        # å¯è¦–åŒ–ï¼ˆPC1Ã—PC2ï¼‰
        fig, ax = plt.subplots(figsize=(9, 7), dpi=120)
        xy = scores_df[["PC1","PC2"]].values
        ax.scatter(xy[:,0], xy[:,1], s=60, alpha=0.9)
        for i, name in enumerate(df_raw["åº—å"].astype(str).values):
            if i < len(xy):
                ax.annotate(name, (xy[i,0], xy[i,1]), xytext=(4,4), textcoords="offset points", fontsize=9)
        ax.axhline(0, lw=1, color="gray", alpha=0.6)
        ax.axvline(0, lw=1, color="gray", alpha=0.6)
        ax.set_xlabel(f"PC1 ({ev_ratio[0]*100:.1f}% var)")
        ax.set_ylabel(f"PC2 ({ev_ratio[1]*100:.1f}% var)")
        ax.set_title("PCA ãƒãƒƒãƒ—ï¼ˆåº—èˆ—ã®ä½ç½®ï¼šPC1Ã—PC2ï¼‰")
        st.pyplot(fig, clear_figure=True)

        # ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆè² è·é‡ï¼‰
        if show_vectors and "PC1" in loadings.columns and "PC2" in loadings.columns:
            fig2, ax2 = plt.subplots(figsize=(9, 7), dpi=120)
            ax2.axhline(0, lw=1, color="gray", alpha=0.6)
            ax2.axvline(0, lw=1, color="gray", alpha=0.6)
            ax2.set_xlim(-1.1, 1.1); ax2.set_ylim(-1.1, 1.1)
            ax2.set_xlabel("PC1 loading"); ax2.set_ylabel("PC2 loading")
            ax2.set_title("é …ç›®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆè² è·é‡ï¼‰")
            L = loadings[["PC1","PC2"]].copy()
            L["_mag"] = np.sqrt(L["PC1"]**2 + L["PC2"]**2)
            L = L.sort_values("_mag", ascending=False).head(max_vec)
            for item, row in L.iterrows():
                ax2.arrow(0,0, row["PC1"], row["PC2"], head_width=0.03, length_includes_head=True, alpha=0.85)
                ax2.text(row["PC1"]*1.05, row["PC2"]*1.05, str(item), fontsize=9)
            st.pyplot(fig2, clear_figure=True)

        # ãƒ†ãƒ¼ãƒ–ãƒ«
        st.subheader("å¯„ä¸ç‡")
        var_df = pd.DataFrame({
            "PC": [f"PC{i+1}" for i in range(len(ev_ratio))],
            "å›ºæœ‰å€¤": ev,
            "å¯„ä¸ç‡": ev_ratio,
            "ç´¯ç©å¯„ä¸ç‡": ev_ratio.cumsum()
        })
        st.dataframe(var_df.style.format({"å›ºæœ‰å€¤":"{:.3f}","å¯„ä¸ç‡":"{:.3%}","ç´¯ç©å¯„ä¸ç‡":"{:.3%}"}), use_container_width=True)

        st.subheader("è² è·é‡ï¼ˆé …ç›®Ã—PCï¼‰")
        st.dataframe(loadings.style.format("{:.3f}"), use_container_width=True)

        st.subheader("åº—èˆ—ã‚¹ã‚³ã‚¢ï¼ˆPCåº§æ¨™ï¼‰")
        out_scores = pd.concat([df_raw[["åº—å","æ—¥ä»˜"]].reset_index(drop=True),
                                scores_df.reset_index(drop=True)], axis=1)
        st.dataframe(out_scores, use_container_width=True)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.download_button("PCA_è² è·é‡.csv ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                           loadings.to_csv().encode("utf-8-sig"),
                           file_name="pca_loadings.csv", mime="text/csv")
        st.download_button("PCA_åº—èˆ—ã‚¹ã‚³ã‚¢.csv ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                           out_scores.to_csv(index=False).encode("utf-8-sig"),
                           file_name="pca_scores_by_store.csv", mime="text/csv")

        # å‚è€ƒï¼šæ—§ãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆONã®æ™‚ã ã‘ãƒã‚§ãƒƒã‚¯ï¼‰
        if show_matrix:
            required = set(DIVERSITY_COLS) | set(BRAND_COLS)
            missing = [c for c in required if c not in df_raw.columns]
            if missing:
                st.warning(
                    "æ—§ãƒãƒˆãƒªã‚¯ã‚¹ç”¨ã®åˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: " + ", ".join(missing) +
                    "\nğŸ’¡ 1è¡Œç›®è¦‹å‡ºã—ã‚’æ­£ç¢ºã«åˆã‚ã›ã‚‹ã‹ã€NORMALIZE_RULESã«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
                )
            else:
                df_old = df_raw.copy()
                df_old["å¤šæ§˜æ€§åˆè¨ˆ"] = df_old[DIVERSITY_COLS].sum(axis=1)
                df_old["é˜²è¡›åˆè¨ˆ"]   = df_old[BRAND_COLS].sum(axis=1)
                draw_matrix_plot(df_old, show_all=show_all, show_labels=show_labels, max_labels=max_labels)

    except Exception as e:
        st.exception(e)
