# app.py â€” Streamlit: Googleãƒ•ã‚©ãƒ¼ãƒ â†’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆâ†’ãƒãƒˆãƒªã‚¯ã‚¹å¯è¦–åŒ–ï¼ˆå®Œå…¨ç‰ˆï¼‰
import os, re, json, unicodedata
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from matplotlib import font_manager, rcParams

import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import get_as_dataframe
from gspread.exceptions import SpreadsheetNotFound, WorksheetNotFound

# ========================= æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ =========================
MEIRYO_PATH = r"/usr/share/fonts/truetype/msttcorefonts/Meiryo.ttf"  # Linuxã®ä¾‹
try:
    if os.path.exists(MEIRYO_PATH):
        font_manager.fontManager.addfont(MEIRYO_PATH)
        rcParams["font.family"] = font_manager.FontProperties(fname=MEIRYO_PATH).get_name()
    else:
        rcParams["font.family"] = "DejaVu Sans"
except Exception:
    rcParams["font.family"] = "DejaVu Sans"
rcParams["axes.unicode_minus"] = False

# ========================= è©•ä¾¡å®šç¾© =========================
DIVERSITY_COLS = [
    "å¤šæ§˜æ€§1_ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§","å¤šæ§˜æ€§2_å†…è£…ã®å€‹æ€§","å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©","å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "å¤šæ§˜æ€§5_åœ°åŸŸæ€§ã®åæ˜ ","å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€","å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•","å¤šæ§˜æ€§8_å®¢å±¤ã®å¤šæ§˜æ€§",
    "å¤šæ§˜æ€§9_æä¾›æ–¹æ³•ã®ç‰¹ç•°æ€§","å¤šæ§˜æ€§10_åº—ã®ç‰©èªæ€§"
]
BRAND_COLS = [
    "é˜²è¡›1_å‘³ã®ä¿¡é ¼æ„Ÿï¼ˆåˆè¨ªï¼‰","é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ","é˜²è¡›3_æ¥å®¢æ…‹åº¦","é˜²è¡›4_ä¾¡æ ¼ã®æ˜ç¢ºã•",
    "é˜²è¡›5_æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰","é˜²è¡›6_æ”¯æ‰•ã„ã®å®‰å…¨æ€§","é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•","é˜²è¡›8_åˆè¦‹å®¢ã¸ã®å¯¾å¿œ",
    "é˜²è¡›9_å¸¸é€£/å£ã‚³ãƒŸ","é˜²è¡›10_ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›"
]
BASE_COLS = ["æ—¥ä»˜","åº—å","å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰"]
REQUIRED_COLS = BASE_COLS + DIVERSITY_COLS + BRAND_COLS

# è¦‹å‡ºã—ã‚†ã‚‰ãå¸åï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹é›†ï¼‰
NORMALIZE_RULES = {
    # ãƒ™ãƒ¼ã‚¹
    "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—",
    "è¨ªå•æ—¥": "æ—¥ä»˜", "ãŠåº—å": "åº—å", "åº—å": "åº—å",
    "step0": "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰", "step 0": "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰",
    "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼": "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰",
    # å¤šæ§˜æ€§
    "ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§": "å¤šæ§˜æ€§1_ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§",
    "å†…è£…ã®å€‹æ€§": "å¤šæ§˜æ€§2_å†…è£…ã®å€‹æ€§",
    "åº—ä¸»": "å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©", "ã‚¹ã‚¿ãƒƒãƒ•": "å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©",
    "ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§": "å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "ã‚µãƒ¼ãƒ“ã‚¹ã®ç‹¬è‡ªæ€§": "å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "ç‹¬è‡ªã‚µãƒ¼ãƒ“ã‚¹": "å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "åœ°åŸŸæ€§": "å¤šæ§˜æ€§5_åœ°åŸŸæ€§ã®åæ˜ ",
    "ã‚¤ãƒ™ãƒ³ãƒˆ": "å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€", "å­£ç¯€": "å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€",
    "sns": "å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•", "ï¼³ï¼®ï¼³": "å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•",
    "å®¢å±¤": "å¤šæ§˜æ€§8_å®¢å±¤ã®å¤šæ§˜æ€§",
    "æä¾›æ–¹æ³•": "å¤šæ§˜æ€§9_æä¾›æ–¹æ³•ã®ç‰¹ç•°æ€§",
    "ç‰©èªæ€§": "å¤šæ§˜æ€§10_åº—ã®ç‰©èªæ€§",
    # é˜²è¡›
    "å‘³ã®ä¿¡é ¼æ„Ÿ": "é˜²è¡›1_å‘³ã®ä¿¡é ¼æ„Ÿï¼ˆåˆè¨ªï¼‰",
    "è¡›ç”Ÿ": "é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ", "æ¸…æ½”": "é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ",
    "æ¥å®¢": "é˜²è¡›3_æ¥å®¢æ…‹åº¦",
    "ä¾¡æ ¼ã®æ˜ç¢ºã•": "é˜²è¡›4_ä¾¡æ ¼ã®æ˜ç¢ºã•",
    "æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰": "é˜²è¡›5_æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰",
    "æ”¯æ‰•ã„": "é˜²è¡›6_æ”¯æ‰•ã„ã®å®‰å…¨æ€§",
    "å…¥åº—ã—ã‚„ã™ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "å…¥åº—ã®ã—ã‚„ã™ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "å…¥ã‚Šã‚„ã™ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "å…¥ã‚Šæ˜“ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "å…¥åº—ã—æ˜“ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "åˆè¦‹å®¢": "é˜²è¡›8_åˆè¦‹å®¢ã¸ã®å¯¾å¿œ",
    "å£ã‚³ãƒŸ": "é˜²è¡›9_å¸¸é€£/å£ã‚³ãƒŸ",
    "ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›": "é˜²è¡›10_ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›",
}

MIDLINE = 30
MIN_SCORE, MAX_SCORE = 1, 5

# ========================= æ­£è¦åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========================
def _norm(s: str) -> str:
    """NFKCæ­£è¦åŒ–â†’å…¨ç©ºç™½é™¤å»â†’å°æ–‡å­—åŒ–ï¼ˆãƒãƒƒãƒç”¨ï¼‰"""
    s = unicodedata.normalize("NFKC", str(s))
    return s.replace(" ", "").replace("ã€€", "").lower()

def extract_sheet_id(text: str) -> str:
    """URLã§ã‚‚IDã§ã‚‚OKã€‚/d/â€¦/ ã‹ã‚‰æŠ½å‡ºã€‚/edit ãŒç„¡ãã¦ã‚‚å¯¾å¿œã€‚"""
    t = (text or "").strip()
    m = re.search(r"/d/([a-zA-Z0-9-_]+)/?", t)
    return m.group(1) if m else t

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """åˆ—åã‚’ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§æ­£è¦åŒ–ï¼ˆåŒ…å«åˆ¤å®šã¯æ­£è¦åŒ–å¾Œã§é ‘ä¸ˆã«ï¼‰"""
    new_cols = []
    for c in df.columns:
        cc_norm = _norm(c)
        mapped = None
        for key, dest in NORMALIZE_RULES.items():
            if _norm(key) in cc_norm:
                mapped = dest
                break
        new_cols.append(mapped or c)
    df.columns = new_cols
    return df

# ========================= èªè¨¼æƒ…å ± =========================
def build_creds_from_secrets_or_text() -> dict | None:
    """
    Secretsã‚’2æ–¹å¼å¯¾å¿œ:
      A) st.secrets['gcp']['service_account_json'] ã«JSONæ–‡å­—åˆ—
      B) st.secrets['gcp'] ã«å€‹åˆ¥ã‚­ãƒ¼ï¼ˆproject_id ç­‰ï¼‰
    ç„¡ã‘ã‚Œã°è²¼ã‚Šä»˜ã‘UIã‚’å‡ºã™ã€‚
    """
    svc = st.secrets.get("gcp", {})
    # A: JSONä¸¸ã”ã¨
    if "service_account_json" in svc:
        try:
            return json.loads(svc["service_account_json"])
        except Exception as e:
            st.error(f"Secretsã® service_account_json ãŒä¸æ­£ã§ã™: {e}")
            return None
    # B: å€‹åˆ¥ã‚­ãƒ¼
    required = {"type","project_id","private_key_id","private_key","client_email","client_id"}
    if required.issubset(set(svc.keys())):
        return {
            "type": "service_account",
            "project_id": svc["project_id"],
            "private_key_id": svc["private_key_id"],
            "private_key": svc["private_key"],
            "client_email": svc["client_email"],
            "client_id": svc["client_id"],
            "auth_uri": svc.get("auth_uri","https://accounts.google.com/o/oauth2/auth"),
            "token_uri": svc.get("token_uri","https://oauth2.googleapis.com/token"),
            "auth_provider_x509_cert_url": svc.get("auth_provider_x509_cert_url","https://www.googleapis.com/oauth2/v1/certs"),
            "client_x509_cert_url": svc.get("client_x509_cert_url",""),
            "universe_domain": svc.get("universe_domain","googleapis.com"),
        }
    # C: æœªè¨­å®š â†’ è²¼ã‚Šä»˜ã‘æ•‘æ¸ˆ
    with st.expander("ğŸ” ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ï¼ˆSecretsãŒæœªè¨­å®šã®ã¨ãç”¨ï¼‰", expanded=True):
        pasted = st.text_area("Paste JSON", height=180, label_visibility="collapsed")
        if pasted.strip():
            try:
                return json.loads(pasted)
            except Exception as e:
                st.error(f"JSONè§£æã«å¤±æ•—: {e}")
    return None

# ========================= ãƒ‡ãƒ¼ã‚¿å–å¾— =========================
@st.cache_data(show_spinner=False)
def load_sheet(creds_dict: dict, sheet_id: str, worksheet: str) -> pd.DataFrame:
    creds = Credentials.from_service_account_info(
        creds_dict, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
    )
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(sheet_id)
    # worksheetåãŒæ›–æ˜§ãªå¯èƒ½æ€§ã«å°‘ã—å¯„ã‚Šæ·»ã†
    try:
        ws = sh.worksheet(worksheet)
    except WorksheetNotFound:
        # é¡ä¼¼å€™è£œã‚’æ¢ã™
        titles = [w.title for w in sh.worksheets()]
        norm_ws = _norm(worksheet)
        cand = [t for t in titles if _norm(t) == norm_ws or _norm(worksheet) in _norm(t)]
        if cand:
            ws = sh.worksheet(cand[0])
        else:
            raise
    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)
    df = df.dropna(how="all")
    return normalize_columns(df)

# ========================= å‰å‡¦ç† =========================
def coerce_scores(df: pd.DataFrame) -> pd.DataFrame:
    for c in DIVERSITY_COLS + BRAND_COLS:
        s = pd.to_numeric(df[c], errors="coerce").clip(MIN_SCORE, MAX_SCORE).fillna(MIN_SCORE).astype(int)
        df[c] = s
    return df

def deduplicate(df: pd.DataFrame, keys=("åº—å","æ—¥ä»˜"), ts_col="ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—"):
    if all(k in df.columns for k in keys):
        if ts_col in df.columns:
            d = df.copy()
            d["_ts"] = pd.to_datetime(d[ts_col], errors="coerce")
            d = d.sort_values("_ts").drop_duplicates(subset=list(keys), keep="last").drop(columns=["_ts"])
            return d
        return df.drop_duplicates(subset=list(keys), keep="last")
    return df

def compute_totals(df: pd.DataFrame) -> pd.DataFrame:
    df["å¤šæ§˜æ€§åˆè¨ˆ"] = df[DIVERSITY_COLS].sum(axis=1)
    df["é˜²è¡›åˆè¨ˆ"] = df[BRAND_COLS].sum(axis=1)
    mask = df["å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰"].astype(str).str.strip().str.lower().isin(["yes","y","true","1","ok","â—‹"])
    df["_plot_x"] = df["å¤šæ§˜æ€§åˆè¨ˆ"].where(mask)
    df["_plot_y"] = df["é˜²è¡›åˆè¨ˆ"].where(mask)
    return df

# ========================= æç”» =========================
def draw_plot(df: pd.DataFrame):
    fig, ax = plt.subplots(figsize=(8.8, 5.6), dpi=120)
    plot_df = df.dropna(subset=["_plot_x","_plot_y"])
    ax.scatter(plot_df["_plot_x"], plot_df["_plot_y"], s=42)
    ax.axvline(MIDLINE, lw=1); ax.axhline(MIDLINE, lw=1)
    ax.set_xlim(0,50); ax.set_ylim(0,50)
    ax.set_xlabel("å¤šæ§˜æ€§åˆè¨ˆï¼ˆ1ã€œ5Ã—10ï¼10ã€œ50ï¼‰")
    ax.set_ylabel("ãƒ–ãƒ©ãƒ³ãƒ‰é˜²è¡›åˆè¨ˆï¼ˆ1ã€œ5Ã—10ï¼10ã€œ50ï¼‰")
    ax.set_title("é£²é£Ÿåº—ã‚¹ã‚³ã‚¢ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆå‘³OKã®ã¿ãƒ—ãƒ­ãƒƒãƒˆï¼‰")
    st.pyplot(fig, clear_figure=True)

# ========================= UI =========================
st.set_page_config(page_title="é£²é£Ÿåº—ã‚¹ã‚³ã‚¢ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹", layout="wide")
st.title("é£²é£Ÿåº—ã‚¹ã‚³ã‚¢ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆGoogleãƒ•ã‚©ãƒ¼ãƒ  â†’ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰")

# Secrets æ—¢å®šå€¤ï¼ˆã‚ã‚Œã°åˆ©ç”¨ï¼‰
default_sheet_id = st.secrets.get("gcp", {}).get("sheet_id", "")
default_ws = st.secrets.get("gcp", {}).get("worksheet", "Form Responses")

with st.sidebar:
    st.header("è¨­å®š")
    sheet_id_input = st.text_input("Spreadsheet ID / URL", value=default_sheet_id, placeholder="ID ã¾ãŸã¯ URL ã‚’å…¥åŠ›")
    ws_name = st.text_input("Worksheetåï¼ˆã‚¿ãƒ–åï¼‰", value=default_ws, placeholder="ä¾‹ï¼šForm Responses / ãƒ•ã‚©ãƒ¼ãƒ ã®å›ç­” 1")
    dedup_keys = st.text_input("é‡è¤‡é™¤å»ã‚­ãƒ¼ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰", value="åº—å,æ—¥ä»˜")
    ts_col = st.text_input("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ—ï¼ˆä»»æ„ï¼‰", value="ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—")

    # å…±æœ‰æ¼ã‚Œãƒ»IDå–ã‚Šé•ãˆã®å³æ™‚ç¢ºèªç”¨
    creds_preview = build_creds_from_secrets_or_text()
    sid_preview = extract_sheet_id(sheet_id_input or default_sheet_id)
    if creds_preview:
        st.caption(f"ğŸ”‘ SA: {creds_preview.get('client_email','(unknown)')}")
    if sid_preview:
        st.caption(f"ğŸ“„ Sheet ID: {sid_preview}")

    go = st.button("èª­ã¿è¾¼ã¿ï¼†ãƒ—ãƒ­ãƒƒãƒˆ", type="primary")

if go:
    creds = creds_preview or build_creds_from_secrets_or_text()
    if not creds:
        st.stop()

    try:
        sid = extract_sheet_id(sheet_id_input or default_sheet_id)
        if not sid:
            st.error("Spreadsheet ID / URL ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"); st.stop()

        df = load_sheet(creds, sid, ws_name or default_ws)

        # å¿…è¦åˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
        missing = [c for c in REQUIRED_COLS if c not in df.columns]
        if missing:
            st.error(f"å¿…è¦åˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing}")
            st.caption("ğŸ’¡ åˆ—åã®è¡¨è¨˜ã‚†ã‚‰ããŒåŸå› ã®å ´åˆã¯ã€1è¡Œç›®ã®è¦‹å‡ºã—ã‚’æ­£ç¢ºã«åˆã‚ã›ã‚‹ã‹ã€NORMALIZE_RULES ã«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
            st.dataframe(df.head())
            st.stop()

        df = coerce_scores(df)

        keys = tuple([k.strip() for k in (dedup_keys or "åº—å,æ—¥ä»˜").split(",") if k.strip()])
        before = len(df)
        df = deduplicate(df, keys=keys if keys else ("åº—å","æ—¥ä»˜"), ts_col=ts_col or "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—")
        after = len(df)
        st.caption(f"é‡è¤‡é™¤å»: {before - after}ä»¶ï¼ˆã‚­ãƒ¼: {keys if keys else ('åº—å','æ—¥ä»˜')} / ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æœ€æ–°ã‚’æ¡ç”¨ï¼‰")

        df = compute_totals(df)
        draw_plot(df)

        st.dataframe(
            df[BASE_COLS + ["å¤šæ§˜æ€§åˆè¨ˆ","é˜²è¡›åˆè¨ˆ"]].sort_values(["æ—¥ä»˜","åº—å"]),
            use_container_width=True
        )
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        csv = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="scores_cleaned.csv", mime="text/csv")

    except SpreadsheetNotFound as e:
        st.error("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ï¼ˆ404ï¼‰ã€‚IDãŒèª¤ã£ã¦ã„ã‚‹ã‹ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«å…±æœ‰ãŒä»˜ã„ã¦ã„ã¾ã›ã‚“ã€‚"
                 " â†’ å¯¾ç­–: è©²å½“ã‚·ãƒ¼ãƒˆã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤ºã•ã‚ŒãŸ SA ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã« Viewer å…±æœ‰ã—ã¦ãã ã•ã„ã€‚")
        st.exception(e)
    except WorksheetNotFound as e:
        st.error(f"æŒ‡å®šã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆï¼ˆã‚¿ãƒ–ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ws_name}")
        try:
            # å€™è£œã‚’æç¤º
            creds2 = Credentials.from_service_account_info(
                creds, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
            )
            gc2 = gspread.authorize(creds2)
            sh2 = gc2.open_by_key(sid)
            titles = [w.title for w in sh2.worksheets()]
            st.info(f"åˆ©ç”¨å¯èƒ½ãªã‚¿ãƒ–: {titles}")
        except Exception:
            pass
        st.exception(e)
    except Exception as e:
        st.exception(e)

st.markdown("---")
st.write("ğŸ’¡ *Yes* ç³»ã®å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆã€‚å¢ƒç•Œã¯ 30 ç‚¹ï¼ˆ10é …ç›®Ã—3ï¼‰ã€‚è¦‹å‡ºã—ã®ã‚†ã‚‰ãã¯è‡ªå‹•æ­£è¦åŒ–ï¼‹ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§å¸åã—ã¾ã™ã€‚")
