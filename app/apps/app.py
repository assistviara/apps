# app.py â€” Streamlit: Googleãƒ•ã‚©ãƒ¼ãƒ â†’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆâ†’ãƒãƒˆãƒªã‚¯ã‚¹å¯è¦–åŒ–ï¼ˆå®Œå…¨ç‰ˆãƒ»åº—åãƒ©ãƒ™ãƒ«ä»˜ãï¼‰
import os, re, json, unicodedata
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from matplotlib import font_manager, rcParams
from matplotlib.patches import Rectangle

import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import get_as_dataframe
from gspread.exceptions import SpreadsheetNotFound, WorksheetNotFound
from pathlib import Path

# ========================= æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æœ‰åŠ¹åŒ–ï¼ˆåŒæ¢±ãƒ•ã‚©ãƒ³ãƒˆå„ªå…ˆï¼‰ =========================
FONT_DIR = Path(__file__).parent / "fonts"
JP_FONT = FONT_DIR / "NotoSansJP-Regular.ttf"

try:
    if JP_FONT.exists():
        font_manager.fontManager.addfont(str(JP_FONT))
        try:
            font_manager._rebuild()
        except Exception:
            pass
        jp_name = font_manager.FontProperties(fname=str(JP_FONT)).get_name()
        rcParams["font.family"] = "sans-serif"
        rcParams["font.sans-serif"] = [jp_name, "DejaVu Sans", "Arial", "Liberation Sans"]
    else:
        rcParams["font.family"] = "DejaVu Sans"
    rcParams["axes.unicode_minus"] = False
except Exception:
    rcParams["font.family"] = "DejaVu Sans"
    rcParams["axes.unicode_minus"] = False

# ========================= è©•ä¾¡å®šç¾© =========================
DIVERSITY_COLS = [
    "å¤šæ§˜æ€§1_ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§","å¤šæ§˜æ€§2_å†…è£…ã®å€‹æ€§","å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©","å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "å¤šæ§˜æ€§5_åœ°åŸŸæ€§ã®åæ˜ ","å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€","å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•","å¤šæ§˜æ€§8_å®¢å±¤ã®å¤šæ§˜æ€§",
    "å¤šæ§˜æ€§9_æä¾›æ–¹æ³•ã®ç‰¹ç•°æ€§","å¤šæ§˜æ€§10_åº—ã®ç‰©èªæ€§"
]
BRAND_COLS = [
    "é˜²è¡›1_å‘³ã®ä¿¡é ¼æ„Ÿï¼ˆåˆè¨ªï¼‰","é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ","é˜²è¡›3_æ¥å®¢æ…‹åº¦","é˜²è¡›4_ä¾¡æ ¼ã®æ˜ç¢ºã•",
    "é˜²è¡›5_æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰","é˜²è¡›6_æ”¯æ‰•ã„ã®å®‰å…¨æ€§","é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•","é˜²è¡›8_åˆè¦‹å®¢ã¸ã®å¯¾å¿œ",
    "é˜²è¡›9_å¸¸é€£/å£ã‚³ãƒŸ","é˜²è¡›10_ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›"
]
BASE_COLS = ["æ—¥ä»˜","åº—å","å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰"]
REQUIRED_COLS = BASE_COLS + DIVERSITY_COLS + BRAND_COLS

# è¦‹å‡ºã—ã‚†ã‚‰ãå¸åï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹é›†ï¼‰
NORMALIZE_RULES = {
    # ãƒ™ãƒ¼ã‚¹
    "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—",
    "è¨ªå•æ—¥": "æ—¥ä»˜", "ãŠåº—å": "åº—å", "åº—å": "åº—å",
    "step0": "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰", "step 0": "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰",
    "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼": "å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰",
    # å¤šæ§˜æ€§
    "ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§": "å¤šæ§˜æ€§1_ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç‹¬è‡ªæ€§",
    "å†…è£…ã®å€‹æ€§": "å¤šæ§˜æ€§2_å†…è£…ã®å€‹æ€§",
    "åº—ä¸»": "å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©", "ã‚¹ã‚¿ãƒƒãƒ•": "å¤šæ§˜æ€§3_åº—ä¸»ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚­ãƒ£ãƒ©",
    "ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§": "å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "ã‚µãƒ¼ãƒ“ã‚¹ã®ç‹¬è‡ªæ€§": "å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "ç‹¬è‡ªã‚µãƒ¼ãƒ“ã‚¹": "å¤šæ§˜æ€§4_ã‚µãƒ¼ãƒ“ã‚¹ç‹¬è‡ªæ€§",
    "åœ°åŸŸæ€§": "å¤šæ§˜æ€§5_åœ°åŸŸæ€§ã®åæ˜ ",
    "ã‚¤ãƒ™ãƒ³ãƒˆ": "å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€", "å­£ç¯€": "å¤šæ§˜æ€§6_ã‚¤ãƒ™ãƒ³ãƒˆ/å­£ç¯€",
    "sns": "å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•", "ï¼³ï¼®ï¼³": "å¤šæ§˜æ€§7_SNSã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã•",
    "å®¢å±¤": "å¤šæ§˜æ€§8_å®¢å±¤ã®å¤šæ§˜æ€§",
    "æä¾›æ–¹æ³•": "å¤šæ§˜æ€§9_æä¾›æ–¹æ³•ã®ç‰¹ç•°æ€§",
    "ç‰©èªæ€§": "å¤šæ§˜æ€§10_åº—ã®ç‰©èªæ€§",
    # é˜²è¡›
    "å‘³ã®ä¿¡é ¼æ„Ÿ": "é˜²è¡›1_å‘³ã®ä¿¡é ¼æ„Ÿï¼ˆåˆè¨ªï¼‰",
    "è¡›ç”Ÿ": "é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ", "æ¸…æ½”": "é˜²è¡›2_è¡›ç”Ÿ/æ¸…æ½”æ„Ÿ",
    "æ¥å®¢": "é˜²è¡›3_æ¥å®¢æ…‹åº¦",
    "ä¾¡æ ¼ã®æ˜ç¢ºã•": "é˜²è¡›4_ä¾¡æ ¼ã®æ˜ç¢ºã•",
    "æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰": "é˜²è¡›5_æä¾›ã‚¹ãƒ”ãƒ¼ãƒ‰",
    "æ”¯æ‰•ã„": "é˜²è¡›6_æ”¯æ‰•ã„ã®å®‰å…¨æ€§",
    "å…¥åº—ã—ã‚„ã™ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "å…¥åº—ã®ã—ã‚„ã™ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "å…¥ã‚Šã‚„ã™ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "å…¥ã‚Šæ˜“ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "å…¥åº—ã—æ˜“ã•": "é˜²è¡›7_å…¥åº—ã—ã‚„ã™ã•",
    "åˆè¦‹å®¢": "é˜²è¡›8_åˆè¦‹å®¢ã¸ã®å¯¾å¿œ",
    "å£ã‚³ãƒŸ": "é˜²è¡›9_å¸¸é€£/å£ã‚³ãƒŸ",
    "ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›": "é˜²è¡›10_ãƒªã‚¹ã‚¯å¯¾å¿œåŠ›",
}

MIDLINE = 30
MIN_SCORE, MAX_SCORE = 1, 5

# ========================= æ­£è¦åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========================
def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", str(s))
    return s.replace(" ", "").replace("ã€€", "").lower()

def extract_sheet_id(text: str) -> str:
    t = (text or "").strip()
    m = re.search(r"/d/([a-zA-Z0-9-_]+)/?", t)
    return m.group(1) if m else t

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    new_cols = []
    for c in df.columns:
        cc_norm = _norm(c)
        mapped = None
        for key, dest in NORMALIZE_RULES.items():
            if _norm(key) in cc_norm:
                mapped = dest
                break
        new_cols.append(mapped or c)
    df.columns = new_cols
    return df

# ========================= èªè¨¼æƒ…å ± =========================
def build_creds_from_secrets_or_text() -> dict | None:
    svc = st.secrets.get("gcp", {})
    if "service_account_json" in svc:
        try:
            return json.loads(svc["service_account_json"])
        except Exception as e:
            st.error(f"Secretsã® service_account_json ãŒä¸æ­£ã§ã™: {e}")
            return None
    required = {"type","project_id","private_key_id","private_key","client_email","client_id"}
    if required.issubset(set(svc.keys())):
        return {
            "type": "service_account",
            "project_id": svc["project_id"],
            "private_key_id": svc["private_key_id"],
            "private_key": svc["private_key"],
            "client_email": svc["client_email"],
            "client_id": svc["client_id"],
            "auth_uri": svc.get("auth_uri","https://accounts.google.com/o/oauth2/auth"),
            "token_uri": svc.get("token_uri","https://oauth2.googleapis.com/token"),
            "auth_provider_x509_cert_url": svc.get("auth_provider_x509_cert_url","https://www.googleapis.com/oauth2/v1/certs"),
            "client_x509_cert_url": svc.get("client_x509_cert_url",""),
            "universe_domain": svc.get("universe_domain","googleapis.com"),
        }
    with st.expander("ğŸ” ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ï¼ˆSecretsãŒæœªè¨­å®šã®ã¨ãç”¨ï¼‰", expanded=True):
        pasted = st.text_area("Paste JSON", height=180, label_visibility="collapsed")
        if pasted.strip():
            try:
                return json.loads(pasted)
            except Exception as e:
                st.error(f"JSONè§£æã«å¤±æ•—: {e}")
    return None

# ========================= ãƒ‡ãƒ¼ã‚¿å–å¾— =========================
@st.cache_data(show_spinner=False)
def load_sheet(creds_dict: dict, sheet_id: str, worksheet: str) -> pd.DataFrame:
    creds = Credentials.from_service_account_info(
        creds_dict, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
    )
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(sheet_id)
    try:
        ws = sh.worksheet(worksheet)
    except WorksheetNotFound:
        titles = [w.title for w in sh.worksheets()]
        norm_ws = _norm(worksheet)
        cand = [t for t in titles if _norm(t) == norm_ws or _norm(worksheet) in _norm(t)]
        if cand:
            ws = sh.worksheet(cand[0])
        else:
            raise
    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)
    df = df.dropna(how="all")
    return normalize_columns(df)

# ========================= å‰å‡¦ç† =========================
def coerce_scores(df: pd.DataFrame) -> pd.DataFrame:
    for c in DIVERSITY_COLS + BRAND_COLS:
        s = pd.to_numeric(df[c], errors="coerce").clip(MIN_SCORE, MAX_SCORE).fillna(MIN_SCORE).astype(int)
        df[c] = s
    return df

def deduplicate(df: pd.DataFrame, keys=("åº—å","æ—¥ä»˜"), ts_col="ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—"):
    if all(k in df.columns for k in keys):
        if ts_col in df.columns:
            d = df.copy()
            d["_ts"] = pd.to_datetime(d[ts_col], errors="coerce")
            d = d.sort_values("_ts").drop_duplicates(subset=list(keys), keep="last").drop(columns=["_ts"])
            return d
        return df.drop_duplicates(subset=list(keys), keep="last")
    return df

def compute_totals(df: pd.DataFrame) -> pd.DataFrame:
    df["å¤šæ§˜æ€§åˆè¨ˆ"] = df[DIVERSITY_COLS].sum(axis=1)
    df["é˜²è¡›åˆè¨ˆ"] = df[BRAND_COLS].sum(axis=1)
    return df

# ========================= æç”» =========================
def draw_plot(df: pd.DataFrame, show_all: bool, show_labels: bool, max_labels: int):
    fig, ax = plt.subplots(figsize=(9, 6), dpi=120)

    # å‘³OKã®å®šç¾©ï¼ˆå¢—ã‚„ã—ãŸã‘ã‚Œã°ã“ã“ã«è¿½åŠ ï¼‰
    ok_vals = {"yes","y","true","1","ok","â—‹","ã¯ã„","å¯"}
    mask = df["å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰"].astype(str).str.strip().str.lower().isin(ok_vals)

    plot_df = df.copy() if show_all else df[mask].copy()

    # èƒŒæ™¯ï¼ˆè±¡é™ã‚’è–„ãå¡—ã‚‹ï¼‰
    ax.add_patch(Rectangle((0, 0), 50, 50, facecolor=(0,0,0,0.02), edgecolor="none"))
    ax.add_patch(Rectangle((MIDLINE, 0), 50-MIDLINE, 50, facecolor=(0,0,0,0.04), edgecolor="none"))
    ax.add_patch(Rectangle((0, MIDLINE), 50, 50-MIDLINE, facecolor=(0,0,0,0.04), edgecolor="none"))

    # æ•£å¸ƒå›³
    ax.scatter(plot_df["å¤šæ§˜æ€§åˆè¨ˆ"], plot_df["é˜²è¡›åˆè¨ˆ"], s=64, alpha=0.9, linewidths=0.6, edgecolors="white")

    # äº¤å·®ç·š
    ax.axvline(MIDLINE, lw=1)
    ax.axhline(MIDLINE, lw=1)

    # è»¸ã¨ã‚¿ã‚¤ãƒˆãƒ«
    ax.set_xlim(0, 50); ax.set_ylim(0, 50)
    ax.set_xlabel("å¤šæ§˜æ€§åˆè¨ˆï¼ˆ1ã€œ5Ã—10ï¼10ã€œ50ï¼‰")
    ax.set_ylabel("ãƒ–ãƒ©ãƒ³ãƒ‰é˜²è¡›åˆè¨ˆï¼ˆ1ã€œ5Ã—10ï¼10ã€œ50ï¼‰")
    ax.set_title("é£²é£Ÿåº—ã‚¹ã‚³ã‚¢ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆå‘³OKã®ã¿ï¼‰" if not show_all else "é£²é£Ÿåº—ã‚¹ã‚³ã‚¢ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆå…¨ä»¶ï¼‰")

    # ãƒ©ãƒ™ãƒ«ï¼ˆåº—åï¼‰
    if show_labels and not plot_df.empty:
        # ç‚¹æ•°ãŒé«˜ã„é †ã«æœ€å¤§ max_labels ä»¶ã ã‘æ³¨é‡ˆã—ã¦ã€é‡ãªã‚Šã‚’å°‘ã—å›é¿ã™ã‚‹
        label_df = plot_df.sort_values(["å¤šæ§˜æ€§åˆè¨ˆ","é˜²è¡›åˆè¨ˆ"], ascending=False).head(max_labels)
        for _, r in label_df.iterrows():
            ax.annotate(
                str(r["åº—å"]),
                (r["å¤šæ§˜æ€§åˆè¨ˆ"], r["é˜²è¡›åˆè¨ˆ"]),
                xytext=(4, 4), textcoords="offset points", fontsize=9
            )
        if len(plot_df) > max_labels:
            st.caption(f"â€» ãƒ©ãƒ™ãƒ«ã¯ {max_labels} ä»¶ã¾ã§è¡¨ç¤ºï¼ˆå…¨{len(plot_df)}ä»¶ä¸­ï¼‰ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å¤‰æ›´ã§ãã¾ã™ã€‚")

    st.caption(f"ãƒ—ãƒ­ãƒƒãƒˆæ•°: {len(plot_df)} / å…¨ä½“: {len(df)}ï¼ˆ{'å…¨ä»¶' if show_all else 'å‘³OKã®ã¿'}ï¼‰")
    st.pyplot(fig, clear_figure=True)

    # ä¸‹ã«ã€Œåº—åã¨åº§æ¨™ã€ã®è¡¨ã‚’å‡ºã™ï¼ˆå ´æ‰€ãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«ï¼‰
    shown = plot_df.loc[:, ["åº—å","å¤šæ§˜æ€§åˆè¨ˆ","é˜²è¡›åˆè¨ˆ"]].sort_values(["é˜²è¡›åˆè¨ˆ","å¤šæ§˜æ€§åˆè¨ˆ"], ascending=False)
    st.dataframe(shown, use_container_width=True)

# ========================= UI =========================
st.set_page_config(page_title="é£²é£Ÿåº—ã‚¹ã‚³ã‚¢ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹", layout="wide")
st.title("é£²é£Ÿåº—ã‚¹ã‚³ã‚¢ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆGoogleãƒ•ã‚©ãƒ¼ãƒ  â†’ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰")

# Secrets æ—¢å®šå€¤ï¼ˆã‚ã‚Œã°åˆ©ç”¨ï¼‰
default_sheet_id = st.secrets.get("gcp", {}).get("sheet_id", "")
default_ws = st.secrets.get("gcp", {}).get("worksheet", "Form Responses")

with st.sidebar:
    st.header("è¨­å®š")
    sheet_id_input = st.text_input("Spreadsheet ID / URL", value=default_sheet_id, placeholder="ID ã¾ãŸã¯ URL ã‚’å…¥åŠ›")
    ws_name = st.text_input("Worksheetåï¼ˆã‚¿ãƒ–åï¼‰", value=default_ws, placeholder="ä¾‹ï¼šForm Responses / ãƒ•ã‚©ãƒ¼ãƒ ã®å›ç­” 1")
    dedup_keys = st.text_input("é‡è¤‡é™¤å»ã‚­ãƒ¼ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰", value="åº—å,æ—¥ä»˜")
    ts_col = st.text_input("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ—ï¼ˆä»»æ„ï¼‰", value="ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—")

    # è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
    show_all = st.checkbox("å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç„¡è¦–ï¼ˆå…¨ã¦ãƒ—ãƒ­ãƒƒãƒˆï¼‰", value=False)
    show_labels = st.checkbox("åº—åãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤º", value=True)
    max_labels = st.slider("ãƒ©ãƒ™ãƒ«æœ€å¤§ä»¶æ•°", min_value=0, max_value=200, value=50, step=5)

    # å…±æœ‰æ¼ã‚Œãƒ»IDå–ã‚Šé•ãˆã®å³æ™‚ç¢ºèªç”¨
    creds_preview = build_creds_from_secrets_or_text()
    sid_preview = extract_sheet_id(sheet_id_input or default_sheet_id)
    if creds_preview:
        st.caption(f"ğŸ”‘ SA: {creds_preview.get('client_email','(unknown)')}")
    if sid_preview:
        st.caption(f"ğŸ“„ Sheet ID: {sid_preview}")

    go = st.button("èª­ã¿è¾¼ã¿ï¼†ãƒ—ãƒ­ãƒƒãƒˆ", type="primary")

if go:
    creds = creds_preview or build_creds_from_secrets_or_text()
    if not creds:
        st.stop()

    try:
        sid = extract_sheet_id(sheet_id_input or default_sheet_id)
        if not sid:
            st.error("Spreadsheet ID / URL ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"); st.stop()

        df = load_sheet(creds, sid, ws_name or default_ws)

        # å¿…è¦åˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
        missing = [c for c in REQUIRED_COLS if c not in df.columns]
        if missing:
            st.error(f"å¿…è¦åˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing}")
            st.caption("ğŸ’¡ åˆ—åã®è¡¨è¨˜ã‚†ã‚‰ããŒåŸå› ã®å ´åˆã¯ã€1è¡Œç›®ã®è¦‹å‡ºã—ã‚’æ­£ç¢ºã«åˆã‚ã›ã‚‹ã‹ã€NORMALIZE_RULES ã«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
            st.dataframe(df.head())
            st.stop()

        df = coerce_scores(df)

        keys = tuple([k.strip() for k in (dedup_keys or "åº—å,æ—¥ä»˜").split(",") if k.strip()])
        before = len(df)
        df = deduplicate(df, keys=keys if keys else ("åº—å","æ—¥ä»˜"), ts_col=ts_col or "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—")
        after = len(df)
        st.caption(f"é‡è¤‡é™¤å»: {before - after}ä»¶ï¼ˆã‚­ãƒ¼: {keys if keys else ('åº—å','æ—¥ä»˜')} / ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æœ€æ–°ã‚’æ¡ç”¨ï¼‰")

        df = compute_totals(df)
        draw_plot(df, show_all=show_all, show_labels=show_labels, max_labels=max_labels)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ•´å½¢æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’è½ã¨ã›ã‚‹ã‚ˆã†ã«ï¼‰
        out = df.copy()
        out["å‘³OK"] = out["å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¿…è¦æ¡ä»¶ï¼‰"].astype(str)
        csv = out.to_csv(index=False).encode("utf-8-sig")
        st.download_button("CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="scores_cleaned.csv", mime="text/csv")

    except SpreadsheetNotFound as e:
        st.error("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ï¼ˆ404ï¼‰ã€‚IDãŒèª¤ã£ã¦ã„ã‚‹ã‹ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«å…±æœ‰ãŒä»˜ã„ã¦ã„ã¾ã›ã‚“ã€‚"
                 " â†’ å¯¾ç­–: è©²å½“ã‚·ãƒ¼ãƒˆã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤ºã•ã‚ŒãŸ SA ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã« Viewer å…±æœ‰ã—ã¦ãã ã•ã„ã€‚")
        st.exception(e)
    except WorksheetNotFound as e:
        st.error(f"æŒ‡å®šã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆï¼ˆã‚¿ãƒ–ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ws_name}")
        try:
            creds2 = Credentials.from_service_account_info(
                creds, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
            )
            gc2 = gspread.authorize(creds2)
            sh2 = gc2.open_by_key(sid)
            titles = [w.title for w in sh2.worksheets()]
            st.info(f"åˆ©ç”¨å¯èƒ½ãªã‚¿ãƒ–: {titles}")
        except Exception:
            pass
        st.exception(e)
    except Exception as e:
        st.exception(e)

st.markdown("---")
st.write("ğŸ’¡ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ *Yes* ç³»ã®å‘³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆã€‚å·¦ã®ãƒã‚§ãƒƒã‚¯ã§å…¨ä»¶è¡¨ç¤ºã€ãƒ©ãƒ™ãƒ«ä»¶æ•°ã‚‚èª¿æ•´ã§ãã¾ã™ã€‚å¢ƒç•Œã¯ 30 ç‚¹ï¼ˆ10é …ç›®Ã—3ï¼‰ã€‚")
